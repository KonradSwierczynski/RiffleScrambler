\chapter{Analiza problemu}
\thispagestyle{chapterBeginStyle}
\label{rozdzial1}

\section{Wprowadzenie}

Początkowo, w latach 60. XX wieku, hasła przechowywane były w postaci jawnej.
W latach siedemdziesiątych zaczęto stosować funkcję \texttt{crypt}, początkowo opartą na symulacji mechanicznej maszyny szyfrującej M-209, a w kolejnych wersjach opartą na szyfrowaniu DES oraz opartą na funkcji jednokierunkowej. Metoda z szyfrowanie pozwalała odszyfrować hasła osobie znającej klucz, a oby dwie metody pozwalały znaleźć użytkowników z takim samym hasłem, co umożliwiało tworzenie bazu haseł i odpowiadającym im wynikom.
Wprowadzono więc parametr nazwany solą (ang. \textit{salt}), który był losowo wybierany dla każdego hasła, a więc z dużym prawdopodobieństwem takie same hasła były przechowywane jako inne wartości.
Ponieważ hasła nie są idealnie losowe, wystarczy obliczać funkcje tylko dla najczęściej używanych ciągów znaków, zamiast tak jak w ataku \textit{brute force} dla wszystkich.
Gdy adwersarz zdobył plik z przechowywanym hashami, mógł dla każdego hasha próbować znaleźć odpowiadające mu hasło obliczając funkcję dla wszystkich pozycji w słowniku najczęściej występujących haseł.

Aby zapobiec tego rodzaju atakom, zaczęto stosować funkcje hashujące na tyle ciężkie do obliczenia, alby atak słownikowy był jak najbardziej niepraktyczny. 
Ponieważ funkcja hashująca musi być obliczana podczas każdego uwierzytelniania w celu sprawdzenia poprawności hasła, nie może być ona zbyt ciężka do obliczenia dla aplikacji uwierzytelniającej.
Z drugiej strony, gdy krotka ($login$, $y$, $salt$) wycieknie, adwersarz przeprowadzający atak słownikowy obliczając funkcję hashującą dla każdej wartości ze słownik, co trzeba uczynić jak najbardziej kosztowne.
Dobra funkcja do przechowywania haseł powinna być tak samo kosztowna do obliczenia dla aplikacji uwierzytelniającej jak dla adwersarza.

W tym celu zaczęto stosować funkcje, które obliczają wiele razy kryptograficzną funkcję skrótu. Przykładem takiej funkcji jest PBKDF2 \cite{pbkdf2} (ang. \textit{Pssowrd-Based Key Derivation Function 2}), dla której zalecanym parametrem bezpieczeństwa w 2000 roku było 1024 iteracji, a już w 2005 zaczęto zalecać 4096 iteracji, z powodu wzrostu wydajności CPU.

Takie podejście nie gwarantuje zabezpieczenia przed adwersarzem używającym sepcjalizowany układ scalony (ang. \textit{ASIC - Application-Specyfic Integrated Circut}).
Układy takie są znacznie bardziej wydajne poz względem szybkości obliczania funkcji skrótu takich jak SHA256 czy MD5 niż tradycyjne architektury, a więc zwiększenie bezpieczeństwa za pomocą parametrów obciąży aplikację uwierzytelniającą w dużo większym stopniu, niż adwersarza.
Dla przykładu Antimiser S9 \cite{antminer} potrafi w jednej sekundzie obliczyć $1.4\times 10^{13}$ wyników (dla funkcji double SHA256), podczas gdy obecnie dostępne karty graficzne są w stanie liczyć taką funkcję z prędkością $3\times 10^{10}$ wyników na sekundę, a procesory z prędkością około $10^9$ wyników na sekundę \cite{rs}.

Zauważono jednak, że na różnych architekturach koszt dostępu do pamięci jest dużo bardziej zrównoważony niż koszt obliczeń. \cite{percival2009stronger}.
Zaproponowano więc memory-hard functions (MHF), które wywołują podczas obliczania wiele kosztownych czasowo odwołań do pamięci.

O MHF można myśleć jako o pewnej kolejności dostępu do komórek pamięci. Odwołania następują do już wcześniej obliczonych wartości w komórkach.
Zatem kolejność tą można opisać jako acykliczny graf skierowany (DAG, ang. \textit{directed acyclic graph}).

MKF możemy oznaczyć jako funkcję z dostępem do pamięci zależnym od danych dMHF(ang. \textit{data-dependent MHF}) oraz z dostępem do pamięci niezależnym od danych iMHF(ang. \textit{data-independent MHF})). Ponieważ dostęp do pamięci oparty na podanym haśle może umożliwiać przeprowadzenia ataków na przykład opartych na kolejności dostępu do pamięci, w pracy omówione są jedynie funkcjie typu iMHF.

Istnieje już wiele funkcji typu \textit{memory-hard}. Jednymi z najbardziej popularnych, a co za tym idzie, najlepiej zbadanych są Argon2 \cite{biryukov2016argon2}, Catena \cite{forler2013catena} oraz Ballon Hashing \cite{boneh2016balloon}.

\section{Notacja}

W dalszej części używana będzie następująca notacja.
Zbiory $\mathbb{N} = \{ 0,1,2,\dots \}$, $ \mathbb{N}^{+} = \{ 1, 2, \dots \}$.
$[c] := \{1, 2,\dots,c \}$ oraz $[b, c] = \{ b, b+1, \dots, c \}$, gdzie $b, c \in \mathbb{N}$ oraz $b \leq c$.

Skierowany graf acykliczny (ang. \textit{directed acyclic graph}, DAG) $G = (V, E)$ jest rozmiaru $n$ jeżeli $|V| = n$.
Wierzchołek $v \in V$ ma stopień wchodzący $\delta$ równy największemu stopniu wchodzącemu wśród jego wierzchołków $\delta = \mathbf{indeg}(v)$, jeżeli istnieje $\delta$ wchodzących krawędzi $\delta = | \left( V \times {v} \right) \cap E|$.
Graf $G$ ma stopień wchodzący $ \delta = \mathbf{indeg}(G) = \max_{v \in V} \mathbf{indeg}(v)$.
Wierzchołki o stopniu wchodzącym 0 nazywane są źródłami, a wierzchołki bez krawędzi wychodzących nazywane są ujściami.

Zbiór rodziców wierzchołka $v \in V$ oznaczany jest jako $\mathbf{parents}_{G}(v) = \{ u \in V: (u, v) \in E \}$.
Uogólniając, zbiór przodków $v$ oznaczany jest jako $\mathbf{ancestors}_{G}(v) = \bigcup_{i \geq 1}\mathbf{parents}_{G}^{i}(v)$, przyjmując $\mathbf{parents}_{G}^{i+1}(v) = \mathbf{parents}_{G}(\mathbf{parents}_{G}^{i}(v)))$. Jeżeli wybór grafu $G$ wynika z kontekstu, będziemy oznaczać te zbiory jako $\mathbf{parents}$ oraz $\mathbf{ancestors}$.

Zbiór wszystkich ujść w grafie $G$ oznaczany jest jako $\mathbf{sinks}(G) = \{ v \in V : \nexists (v, u) \in E \}$. DAG G, który jest spójny, a tylko takie będą rozważane w dalszej części pracy, zachowuje równość $\mathbf{ancestors}(\mathbf{sinks}(G)) = V$.

Dla skierowanej ścieżki $p = (v_{1},v_{2},\dots,v_{z})$ w $G$, jej długość jest równa ilości wierzchołku przez które przechodzi $\mathbf{length}(p) := z$.
Mając DAG G, oznaczamy długość jego najdłuższej ścieżki jako $\mathbf{depth}(G)$.

Mając podzbiór wierzchołków grafu $S \subset V$, poprzez $G - S$ oznaczać będziemy DAG otrzymany z $G$ poprzez usunięcie wierzchołków z $S$ oraz krawędzi wychodzących lub wchodzących do wierzchołków z $S$.

\section{Etykietowanie grafu}

\begin{definition} [\textit{Równoległe/sekwencyjne etykietowanie grafu}] Niech $G = (V, E)$ będzie grafem skierowanym grafem acyklicznym i niech $T \subset V$ będzie zbiorem wierzchołków do oetykietowania. $T$ będzie nazywane celem.
	Stanem etykietowania $G$ jest zbiór $P_{i} \subset V$.
	Poprawnym etykietowaniem równoległym jest ciąg $P = (P_{0}, \dots , P_{t})$ stanów etykietowania $G$,
	gdzie $P_{0} = \emptyset $ oraz gdzie spełnione są warunki 1 oraz 2 poniżej.
	Etykietowanie sekwencyjne musi dodatkowo spełniać warunek 3.
	\begin{enumerate}
		\item Każdy wierzchołek z celu jest w pewnej konfiguracji oetykietowany (nie koniecznie wszystkie jednocześnie).
		$$ \forall x \in T \exists x \leq t : x \in P_{x} $$
		
		\item Oetykietować wierzchołek można tylko wtedy, gdy wszyscy jego rodzice
		są oetykietowani w poprzednim kroku.
		$$ \forall i \in [t] : x \in (P_{i} \setminus P_{i-1}) \Rightarrow \mathbf{parents}(x) \subset P_{i-1} $$
		
		\item W każdym kroku można oetykietować co najwyżej jeden wierzchołek.
		$$ \forall i \in [t]: | P_{i} \setminus P_{i-1} | \leq 1 $$
	\end{enumerate}
	Zbiory poprawnych etykietowań sekwencyjnych i równoległych grafu G z celem T oznaczamy odpowiednio jako 
	$ \mathcal{P}_{G,T} $ oraz $ \mathcal{P}_{G,T}^{ \parallel } $.
	Etykietowania najbardziej interesujących przypadków, gdy $T = \mathbf{sinks}(G)$, oznaczamy $ \mathcal{P}_{G} $ oraz $ \mathcal{P}_{G}^{ \parallel } $.
\end{definition}


 Można zauważyć, że $ \mathcal{P}_{G,T} \subset  \mathcal{P}_{G,T}^{ \parallel } $.

\begin{definition}
	Złożoność czasową (ang. \textit{time}, t), pamięciową (ang. \textit{space}, s), pamięciowo-czasową (ang. \textit{space-time}, st) oraz łączna (ang. \textit{cumulative}, cc) etykietowania $ P = (P_{0}, \dots , P_{t} ) \in \mathcal{P}_{G}^{ \parallel } $ są zdefiniowane jako
	$$ \Pi_{t}(P) = t, \Pi_{s}(P) = \max_{y \in [t]} | P_{i} |, \Pi_{st}(P) = \Pi_{t}(P) * \Pi_{s}(P), \Pi_{cc}(P) = \sum_{i \in [t]}| P_{i}|$$
	Dla $ \alpha \in \{s, t, st, cc \}$ oraz celu $T \subset V $, złożoności sekwencyjnego oraz równoległego
	etykietowania grafu $G$ definiujemy jako
	$$ \Pi_{ \alpha }(G, T) = \min_{P \in \mathcal{P}_{G,T}} \Pi_{ \alpha } (P) $$
	$$ \Pi_{ \alpha }^{ \parallel }(G, T) = \min_{P \in \mathcal{P}_{G,T}^{ \parallel }} \Pi_{ \alpha } (P) $$
	Kiedy $T = \mathbf{sinks}(G)$, piszemy $ \Pi_{ \alpha }^{ \parallel }(G) $ oraz $ \Pi_{ \alpha }(G) $.
	
\end{definition}

Ponieważ $ \mathcal{P}_{G,T} \subset  \mathcal{P}_{G,T}^{ \parallel } $, dla dowolnej złożoności etykietowania $ \alpha \in \{s, t, st, cc \}$ oraz dowolnego grafu $G$ złożoność etykietowania równoległe jest nie większa, niż złożoność etykietowania sekwencyjnego $ \Pi_{ \alpha }(G) \geq \Pi_{ \alpha}^{\parallel}(G)$, a złożoność łączna jest nie większa, niż czasowo-pamięciowa  $ \Pi_{ st }(G) \geq \Pi_{ cc }(G)$ i $ \Pi_{ st }^{ \parallel }(G) \geq \Pi_{ cc }^{ \parallel }(G)$.

W tej pracy głównie rozważane jest badanie złożoności $\Pi_{ st }$, oraz $\Pi_{ cc }^{ \parallel }$, ponieważ ukazują one kolejno koszt przeprowadzania etykietowania na jednordzeniowej maszynie (np. procesor x86) oraz koszt etykietowania na wyspecjalizowanym układzie.

Aby zobaczyć jakie wartości mogą przyjąć przedstawione złożoności, rozważmy graf rozmiaru $n$.
Każdy graf rozmiaru $n$ może zostać oetykietowany w $n$ krokach, ponieważ ma tylko $n$ wierzchołków. Każdy stan etykietowania nie może również zawierać więcej niż $n$ elementów. Zatem górne ograniczenie możemy przedstawić następująco
$$ \forall G_{n} \in \mathbb{G}_{n} : \Pi_{ cc }^{ \parallel }(G_{n}) \leq \Pi_{ st }(G_{n}) \leq n^{2} . $$

Zobaczmy jak wyglądają złożoności dla grafu pełnego $K_{n} = (V = [n], E= \{
(i,j): 1 \leq i < j \leq n \})$ oraz dla $Q_{n} = (V = [n], E = \{ (i, i+1) : 1 \leq n \} )$.
$$ n(n - 1) / 2 \leq \Pi_{ cc }^{ \parallel }(K_{n}) \leq \Pi_{ st }(K_{n}) \leq n^2$$
Graf $K_{n}$ maksymalizuje złożoności etykietowania, a  $\Pi_{ cc }^{ \parallel }$ jest rożne tylko o stałą od $\Pi_{ st }$. Co oznacza, że koszt obliczania funkcji opartej na takim grafie byłby zdominowany przez koszt dostępu do pamięci, a wyspecjalizowane układy nie dały by dużej przewagi nad tradycyjnym procesorem. Jest to bardzo pożądane dla MHF, jednak ze względu na bardzo wysoki stopień grafu $K_{n}$, nie jest on przydatny przy konstruowaniu takich funkcji.
Stopień wchodzący w grafie $Q_{n}$ jest równy 1, jednak złożoność etykietowania jest bardzo niska
$$ \Pi_{ cc }^{ \parallel }(Q_{n}) = \Pi_{ st }(Q_{n}) \leq n. $$ 
Oznacza to, że koszt obliczania funkcji opartej na takim grafie (taką funkcją jest PBKDF2) nie jest zależny w dużej mierze od kosztu dostępu do pamięci nawet dla dużego $n$. 



\section{Superkoncentrator}
Superkoncentrator jest grafem, w którym moc zbioru przodków dla wierzchołków szybko rośnie wraz z numerem pokolenia. Oznacza to, że zbiór kolejnych wierzchołków, będzie posiadał liczebny zbiór rodziców, co czyni superkoncentrator bardzo przydatnym do konstrukcji MHF.
\begin{definition}[N-Superkoncentrator] Skierowany graf acykliczny $G = (V, E)$ o ustalonym stopniu wchodzącym, $N$ wejściach i
	$N$ wyjściach nazywany jest N-Superkoncentratorem, gdy dla każdego $k \in [N]$
	oraz dla każdej pary podzbiorów $V_{1} \subset V$ $k$ wejść i $V_{2} \subset V$ $k$ wyjść istnieje
	$k$ wierzchołkowo-rozłącznych ścieżek łączących wierzchołki ze zbioru $V_{1}$ z wierzchołkami w $V_{2}$.
\end{definition}

\begin{definition}[(N, $\lambda$)-Superkoncentrator] Niech $G_{i}, i = 0, \dots, \lambda-1$ będą N-Superkoncentratorami.
	Niech graf $G$ będzie połączeniem wyjść $G_{i}$ do odpowiadających wejść w $G_{i+1}$ dla $i = 0, \dots, \lambda - 2$.
	Graf $G$ jest nazywany (N, $\lambda$)-Superkoncentratorem.
\end{definition}

\begin{lemma}[Ograniczenie dolne dla (N, $ \lambda $)-Superkoncentratora] \cite[Lemat 1]{rs}
	Etykietowanie (N, $\lambda$)-Superkoncentratora używając $S \leq N/20$ etykiet, wymaga $T$ kroków, gdzie
	$$ T \geq N \left( \frac{ \lambda N}{64 S} \right) ^{ \lambda }.$$
\end{lemma}

\section{Grafy depth-robust}

Alwen i Blocki w swojej pracy \cite{depth}
pokazali, że istnieje zależność między złożonością etykietowania, a własnością depth-robustness. Na podstawie tej właściwości potrafimy określić dolne i górne ograniczenie $\Pi_{ cc }^{ \parallel }$.

\begin{definition}[Depth-robustness] Dla $n \in \mathbb{N}$ oraz $e, d \in [n]$ DAG $G = (V, E)$
	jest (e, d)-depth-robust, jeżeli
	$$ \forall S \subset V \ | S | \leq e \Rightarrow \mathbf{depth}(G - S) \geq d.$$
\end{definition}

\begin{theorem} \cite[Twierdzenie 4]{depth}
	Niech DAG G będzie (e, d)-depth-robust, wtedy $ \Pi_{ cc }^{ \parallel } > ed$.
\end{theorem}

\begin{definition}
	Jeżeli graf $G$ nie jest (e,d)-depth-roubust to nazywany jest (e,d)-reducible.
\end{definition}

\begin{theorem} \cite[Twierdzenie 10]{depth} \label{1::redu}
	Niech $G \in \mathbb{G}_{n, \delta}$ taki, że $G$ jest (e,d)-reducible. Wtedy
	$$ \Pi_{cc}^{\parallel}(G) = O \left( \min_{g \in [d,n]} \Big \lbrace n \left( \frac{dn}{g} + \delta g + e \right) \Big \rbrace \right) $$
	biorąc $g = \sqrt{ \frac{dn}{g}}$ upraszcza się to do $ \Pi_{cc}^{\parallel}(G) = O \left( n ( \sqrt{dn \delta} + e) \right)$.
\end{theorem}

\section{Grafy rozproszone}

\begin{definition}
	[Zależność, ang. \textit{dependency}] Niech $G = (V, E)$ będzie acyklicznym grafem skierowanym.
	Niech $L \subseteq V$.
	Mówimy, że $L$ ma ($z$, $g$)-dependency jeżeli istnieją wierzchołkowo rozłączne ścieżki $p_{1}, \dots , p_{z}$ kończące się w $L$, gdzie każda jest długości co najmniej $g$.
\end{definition}

\begin{definition}
	[Graf rozproszony, ang. \textit{dispersed}] Niech $g, k \in \mathbb{N}$ i $g \geq k$. DAG G jest nazywany (g, k)-dispersed jeżeli istnieje uporządkowanie jego wierzchołków takie, że następujące warunki są spełnione.
	Niech $[k]$ oznacza ostatnie $k$ wierzchołków o uporządkowaniu G i niech $L_{j} = [jg, (j + 1)g - 1]$ będzie j-tym podprzedziałem.
	Wtedy $\forall j \in [ \lfloor k / g \rfloor ]$ przedział $L_{j}$ ma (g, g)-dependency.
	W ogólności, jeżeli dla $ \epsilon \in (0, 1] $ każdy przedział $L_{j}$ ma tylko ($\epsilon$g, g)-dependency, graf G nazywany jest ($\epsilon$, g, g)-dispersed.
\end{definition}

\begin{definition}
	Acykliczny graf skierowany $G = (V, E)$ nazywany jest ($\lambda$,$\epsilon$,g,k)-dispersed jeżeli istnieje $\lambda \in \mathbb{N}^{+} $ rozłącznych podzbiorów wierzchołków \{$ L_{i} \subseteq V$\},
	każdy o rozmiarze $k$ oraz spełnione są następujące warunki.
	\begin{enumerate}
		\item Dla każdego $L_{i}$ istnieje ścieżka przechodząca przez wszystkie wierzchołki $L_{i}$.
		
		\item Dla ustalonego porządku topologicznego G. Dla każdego $i \in [\lambda]$ niech
		$G_{i}$ będzie podgrafem G, zawierającym wszystkie wierzchołki z G, aż do ostatniego wierzchołka z $L_{i}$. $G_{i}$ jest ($\epsilon$,g,k)-dispersed.
	\end{enumerate}
	Zbiór grafów, które są ($\lambda$, $\epsilon$, g, k)-disperesed oznaczamy jako $\mathbb{D}_{\epsilon,g}^{\lambda,k}$.
\end{definition}

\begin{theorem} \cite[Twierdzenie 6]{depth} \label{1::disp}
	Niech $G \in \mathbb{D}_{\epsilon,g}^{\lambda,k}$.
	$$ \Pi_{cc}^{\parallel}(G) \geq \epsilon \lambda g \left( \frac{k}{2} - g \right) $$
\end{theorem}


\section{Grafy rzędowe}

\begin{definition}[Graf N rzędowy] Niech $n, N \in \mathbb{N}^{+}$ takie, że $N + 1$ dzieli $n$ oraz niech $k=n/(N + 1)$. Mówimy, że graf $G$ jest $N$ rzędowy, jeżeli $G$ zawiera ścieżkę przechodzącą przez $n$ wierzchołków $(v_{1},\dots,v_{n})$ oraz dzieląc go na rzędy $L_{j} = \{v_{jk + 1},\dots,v_{jk + k} \}$, dla $j \in \{0,\dots,N\}$, pozostałe krawędzie łączą wierzchołki z niższego rzędu $L_{j}$ jedynie z wierzchołkami z wyższych rzędów $L_{i}$, $i \in \{ j+1,\dots,N \}$.
\end{definition}



\begin{lemma} \cite[Lemat 4.2]{alwen2016efficiently} \label{1::rze}
	Niech $G$ będzie $N$ rzędowym grafem, wtedy dla dowolnego $t \in \mathbb{N}^{+}$, $G$ jest $(n/t, N t + t - N -1)$-reducible.
\end{lemma}






